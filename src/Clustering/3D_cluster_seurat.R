#!/home/fanyucai/software/R/R-v3.2.0/bin/Rscript
#================================================
#
#         Author: luyao
#          Email: yao.lu@oebiotech.com
#         Create: 2020-06-24 17:51:42
#    Description: -
#
#================================================
#!/usr/bin/env Rscript

rm(list=ls())
#=================================================================================
# function defintion
#=================================================================================
# ' Calculates The Optimum Dimensionality From An Elbow Plot.
# '
# ' A function that calculates the optimum dimensionality from an elbow plot. Involves calculating
# ' slopes between two consecutive points. The optimum dimensionality will be determined as the point at
# ' which slopes no longer change or no visible change occurs to the slopes thereafter. In this function, a dimension
# ' is determined optimal when a slope connecting that point to another is flatter than 10 times the flattest slope
# ' in the plot and the stdev at that point is proximal to the lower limit.
# ' The plot data must be plotted by dims (x-axis) and stdev (y-axis).
# '
# ' @param plot_data An elbow plot data used for detemining dimensionality, generated by calling ElbowPlot() on a Seurat object.
# ' @return Returns an integer that represents the optimal dimensionality that was determined by calculation.
# ' @examples
# ' library(Seurat)
# ' p <- ElbowPlot(PBMC)
# ' FindElbow(p$data)
# '
# ' @import Seurat
# '
# ' @export

CustomCol2 <- function(n){
  my_palette=c(
    "#7fc97f","#beaed4","#fdc086","#386cb0","#f0027f","#a34e3b","#666666","#1b9e77","#d95f02","#7570b3",
    "#d01b2a","#43acde","#efbd25","#492d73","#cd4275","#2f8400","#d9d73b","#aed4ff","#ecb9e5","#813139",
    "#743fd2","#434b7e","#e6908e","#214a00","#ef6100","#7d9974","#e63c66","#cf48c7","#ffe40a","#a76e93",
    "#d9874a","#adc64a","#5466df","#d544a1","#54d665","#5e99c7","#006874","#d2ad2c","#b5d7a5","#9e8442",
    "#4e1737","#e482a7","#6f451d","#2ccfe4","#ae6174","#a666be","#a32b2b","#ffff99","#3fdacb","#bf5b17")
  return(my_palette[n])
}

FindElbow <- function(plot_data) {
  # Prepare all required parameters from plot_data.
  dimensions <- plot_data$dims
  total_dims <- length(dimensions)
  stdev <- plot_data$stdev
  slopes_so_far <- c() # Will keep track of all the slopes (between two consecutive points) calculated.
  last_dim <- length(dimensions)

  # Calculates slopes between every pair of consecutive points.
  for (dim in 2:last_dim) {
    slopes_so_far <- c(slopes_so_far, (stdev[dim] - stdev[dim - 1]))
  }

  # Default dimensionality to return is 1.
  dimensionality <- 1
  nth_point <- 2
  for (slope in slopes_so_far) {
    # After trial and error, these conditions seem to work well to determine the elbow.
    # Note: slopes are negative in this plot, so using max(slopes_so_far).
    # The first condition looks to see if we are approaching a lower limit, but this wouldn't be enough to
    # confidently say that it is, so we need the second condition.
    # The second condition makes sure that the point used to determine the slope is very close to the
    # smallest value, which means that it increases the confidence that this slope is indeed approaching
    # a lower limit.
    if (slope > 10 * max(slopes_so_far) && stdev[nth_point] < min(stdev) + ((max(stdev) - min(stdev)) * 0.05)) {
      dimensionality <- nth_point
      return(dimensionality - 1) # Subtract 1 because we want the point that leads into the lower limit or the
      # flat part of the graph.
    }
    nth_point <- nth_point + 1
  }

  # The function could not determine dimensionality or elbow from the plot.
  return(dimensionality)
}

# do dimension reduction using the specified method
RunDimReduc <- function(
object,
feature.use = NULL,
reduct2.use = NULL,
reduct1.use = NULL,
perplexity = 30,
batch.use = NULL,
npcs.use = NULL,
assay.use = NULL ,
...
){

    if ( !is.null(assay.use) ){
        DefaultAssay(object) = assay.use
    }

    if ( is.null( feature.use ) ){
        feature.use = VariableFeatures(object)
    }

    npcs.use = ifelse(is.null(npcs.use),yes = 30,no = npcs.use)

    if ( tolower(reduct2.use) %in% c("cca", "mnn", "harmony") ){
        if ( is.null( batch.use) ){
            warning("NO batch information specified!The batchid will be used as default!")
            batch.use = "batchid"
        }
        # check the batchid in the metadata column names
        if ( ! batch.use %in% colnames(object@meta.data) ){
            stop("NO specified batch column found in the meta.data slot of seurat object!")
        }
    }

    if ( is.null(reduct1.use) ){
        print("NO primary reduction method specified, PCA will be used as default!")
        reduct1.use = "pca"
    }

    if ( is.null(reduct2.use) ){
        reduct2.use = reduct1.use
    }

    object = switch (tolower(reduct2.use),
                'ica' = RunICA(object, nics = npcs.use,
                                features = feature.use,
                                verbose = F, ...),
                'cca' = do.call("RunCCA",c(SplitObject(object, split.by = batch.use ),
                                list(features = feature.use,
                                renormalize =F, rescale = F ))),
                'pca' = RunPCA(object, npcs = 30,
                                features = feature.use,
                                do.print = F, verbose = F, ...),
                # 'lsi' = RunLSI(object, n = npcs.use,
                #                 features = feature.use,
                #                 do.print = F, verbose = F, ...),
                'lsi' = RunSVD( object, n = npcs.use, features = feature.use,
                                reduction.key = 'LSI_', reduction.name = 'lsi',
                                verbose = F, ...),
                'lda' = RunLDA(object,  slot = "counts",
                                topics = 30, method = "Z-score"),
                'lsa' = RunLSA(object, npcs = npcs.use ),
                'svd' = RunSVD( object, n = npcs.use, features = feature.use,
                                verbose = F, ...),
                'tsne' = RunTSNE(object, reduction = reduct1.use,
                                dim.embed = 3, tsne.method = "Rtsne",
                                features = feature.use, perplexity = perplexity,
                                dims = 1:npcs.use, num_threads = 10,
                                max_iter = 2000, check_duplicates = F, ... ),
                'flt-sne' = RunTSNE(object, reduction = reduct1.use,
                                dim.embed = 2, tsne.method = "FIt-SNE",
                                features = feature.use, dims = 1:npcs.use,
                                fasttsne_path = system("which fast_tsne"),
                                nthreads = 10, max_iter = 2000,
                                check_duplicates = F, ... ),
                'rpca' = RunRobPCA(object, pc.genes = feature.use,
                                use.modified.pcscores = T,
                                do.print = F, npcs = npcs.use, ...),
                'swne' = RunSWNE(object, reduction.use = reduct1.use,
                                dims.use = 1:npcs.use,
                                proj.method = "sammon",
                                batch = as.vector(object[[batch.use]]),
                                var.genes = feature.use,
                                ncores = 20, return.format = "seurat"),
                'harmony' = RunHarmony(object, group.by.var = batch.use, dims.use = npcs.use,
                                theta = 2, plot_convergence = F, nclust = 50,
                                max.iter.cluster = 20, max.iter.harmony = 5, ...),
                'mnn' = RunMnn.Seurat(object, features = feature.use,assay = assay.use,
                                batch = batch.use, ...),
                'fastmnn' = RunFastMNN(SplitObject(object), features = feature.use,assay = assay.use,
                                verbose = T, BPPARAM = MulticoreParam(detectCores()), ...),
                'umap' = RunUMAP(object,dims = 1:npcs.use,verbose = F,
                                reduction = reduct1.use, n.components = 3,...),
                                #features = feature.use, ...),
                'phate' = RunPHATE.Seurat(object, features = feature.use,
                                npcs = npcs.use, ...),
                'fa2' = RunForceAtlas2.Seurat(object, reduction = reduct1.use,
                                features = feature.use, npcs = 1:npcs.use, ...),
                'diffusion' = RunDiffusion.Seurat(object, features = feature.use,
                                dims = 1:npcs.use,
                                reduction = reduct1.use, ...) )

    message(paste0(reduct2.use, " Dimension Reduction finished"))
    return( object )
}


suppressWarnings({
    suppressPackageStartupMessages( library("Seurat") )
    suppressPackageStartupMessages( library("Matrix") )
    suppressPackageStartupMessages( library("optparse") )
    suppressPackageStartupMessages( library("ggplot2") )
    suppressPackageStartupMessages( library("Signac"))
    suppressPackageStartupMessages( library("dplyr"))
    suppressPackageStartupMessages(library("future"))
    suppressPackageStartupMessages( library("OESingleCell"))
    suppressPackageStartupMessages( library("SeuratWrappers"))
    suppressPackageStartupMessages( library("ReductionWrappers"))
    suppressPackageStartupMessages( library("SeuratPlotly"))

})

#=command line parameters setting=============================
option_list = list(
    make_option( c("--input", "-i" ), type = "character",
                 help = "The input exprssion matrix in several possible format."),
    make_option( c("--informat", "-f" ), type = "character", default = "tenx",
                 help = "The indication of type of input expression matrix, the possible type can be:
                        seurat: the seurat object from the clustering results."),
    make_option( c("--components", "-t"), type="integer", default=NULL,
        help="the appropriate number of statistically significant dimension to use for clustering."),
    make_option( c("--perplexity", "-p"), type="integer", default=30,
        help="The value of the perplexity used for tSNE"),
    make_option( c("--output","-o"),type="character", default = "./",
        help="the output directory of Clustering results." ),
    make_option( c("--metadata", "-m" ), type="character", default = NULL,
        help="[OPTIONAL]the additional sample metadata which must include sample id in this assay design."),
    make_option( c("--clusteringuse", "-c" ), type="character", default = "snn",
        help="the clustering methods used to produce cell clusters.
             The option can be kmeans,snn,louvain,leiden,SC3,hdbscan,phenograph etc." ),
    make_option( c("--resolution","-r"), type = "double", default = 0.4,
         help = "vaule used to set the resolution of cluster distiguish,
                 use a value above(below)1.0 if you want to obtain a larger(smaller) number of communities."),
    make_option( c("--pointsize", "-s"), type = "double", default = 1,
         help = "[OPTIONAL]the point size in the plot."),
    make_option( c("--batchid", "-b"), type = "character", default = NULL,
         help = "[OPTIONAL]the batch information column name of sample in the metadata."),
    make_option( c("--reduct1","-d"),type = "character",default="pca",
        help="the primary dimension reduction results used for clustering.
                 Now ica,cca,pca,lsi,mnn,tsne,harmony,Flt-SNE,UMAP,swne,diffusion,phate,fa2 are supported."),
    make_option( c("--reduct2","-D"),type = "character",default=NULL,
        help="the secondary dimension reduction usually used to detect community.
                 Now tsne,Flt-SNE,UMAP,swne,diffusion,phate,fa2 are supported.
                 If NULL supplied, NO secondary reduction will be carried out."),
    make_option( c("--rerun"),type = "logical",default=FALSE,
        help="whether to rerun the primary reduction in case of parameter changes."),
    make_option( c("--assay" ), type="character", default = "RNA",
        help="[OPTIONAL]the assay used to calulation in case of multimodal data."),
    make_option( c("--ident2use", "-q" ), type = "character", default = NULL,
         help = "[OPTIONAL]The column name in cell metadata used as identity of each cell combined with which_cell."),
    make_option( c("--which_cells", "-u" ), type = "character", default = NULL,
        help = "The subset of cluster ids used for subtyping."),
    make_option( c("--ncores", "-j" ), type="integer", default = 10,
        help="the number of CPUs used to improve the performace."),
    make_option( c("--species" ), type="character", default = NULL,
        help="the specie where the samples come from.")
    );
opt_parser = OptionParser(option_list=option_list);
opt = parse_args(opt_parser);

#=================================================================================
#parse the command line parameters
#=================================================================================
if ( is.null(opt$resolution) ){
   print("The clustering resolution value will be set to 0.8 as default.")
   resolution = 0.4
}else{
   resolution = opt$resolution
}

if ( is.null(opt$reduct1 )){
    print("NO primary dimension reduction methods specified, pca will be used!")
    reduct1 = "pca"
}else{
    reduct1 = tolower(opt$reduct1)
}


if ( !is.null(opt$batchid) ){
    batchid = opt$batchid
}else{
    batchid = "batchid"
}

if ( !is.null(opt$reduct2) ){
   reduct2 = tolower(opt$reduct2)
   if ( reduct2 %in% c("pca","cca","harmony","ica", "mnn") ){ #if the prevous reduction is primary reduction
       print( "the scondary reduction should not be one of pca, cca, ica, mnn or harmony if specified!")
       print("Change to tSNE method as default!")
       reduct2 = "tsne"
   }
}else{
    reduct2 = tolower(opt$reduct2)
}

if ( is.null(opt$clusteringuse) ){
    clusteruse = "snn"
}else{
    clusteruse = opt$clusteringuse
}

if ( is.null(opt$output) ){
    print("NO output directory specified,the current directory will be used!")
    output_dir = getwd()
}else{
    if ( file.exists(opt$output) ){
        output_dir = opt$output
    }else{
        output_dir = opt$output
        dir.create(output_dir)
    }
}
output_dir = normalizePath(output_dir )

# setting the cores for parallization
options(future.globals.maxSize= Inf ) # setting the maxumium mermory usage much bigger in case of big data
plan("multicore", workers = min(detectCores(), opt$ncores)) # parallization using specified CPUs start from here
# ####################################################################
if ( !is.null(opt$informat) & !is.null(opt$input) ){
    if ( opt$informat == "seurat" ){
        # the input is a seurat object produced by previous analysis
        singlecell_ob = readRDSMC( opt$input, cores = availableCores()  )
        # if the input seurat object version is less than 3, upgrade it to version 3
        if ( singlecell_ob@version < 3 ){
            singlecell_ob = UpdateSeuratObject(singlecell_ob) #make sure the seurat object match with the latest seurat package
        }

        # change the default assay for reduction if necessary
        if ( !is.null( opt$assay) ){
            DefaultAssay(singlecell_ob) = opt$assay
        }else{
            DefaultAssay(singlecell_ob) = "RNA"
        }

        #update the metedata in the singlecell_ob@meta.data with new additional sample metadata
        if ( !is.null(opt$metadata) ){
            additional_metadata = read.csv(opt$metadata,sep=",",header =T )
            rownames(additional_metadata) = additional_metadata$sampleid
            cellnames = Cells(singlecell_ob)
            sampleidx =  gsub("(_|-)[ATGC]{16,}.*","",cellnames,perl=T) #the index order is the same as the row index of the assay metadata
            #integrate the additional metadata from the assay design
            additional_cell_meta = vector( )
            for ( colidx in colnames(additional_metadata) ){
                additional_cell_meta = cbind(additional_cell_meta, as.vector(additional_metadata[sampleidx, colidx]))
            }
            colnames(additional_cell_meta) = colnames(additional_metadata)
            rownames(additional_cell_meta) = cellnames
            additional_cell_meta = as.data.frame(additional_cell_meta)
            singlecell_ob = AddMetaData( singlecell_ob, metadata = additional_cell_meta)
        }

        #check the value of perplexity
        suppressWarnings({
            if ( is.null(opt$perplexity) ){
                if ( is.null(Misc(singlecell_ob, "perplexity")) ){
                    print( "NO previous perplexity is AVAILABLE, 30 will be used as default.")
                    Misc(singlecell_ob, "perplexity") = 30
                }else{
                    perplexity = Misc(singlecell_ob, "perplexity")
                }
            }else{
                perplexity = opt$perplexity
                Misc(singlecell_ob, "perplexity") = opt$perplexity
            }
        })

        if ( is.null(singlecell_ob@meta.data$clusters) ){
            singlecell_ob = StashIdent(singlecell_ob, save.name = "clusters")
        }else{
            # if it is the first time to run this script on a Seurat object, the
            # clusters here is actually the sample index but cell cluster ids.
            # After running this script, the cluster id will be overwrite with
            # the actual cell cluster id.
            singlecell_ob = SetIdent( singlecell_ob, value = "clusters")
        }

        #get the subset of cells used for visualization if necessay
        if ( !is.null(opt$which_cells)){
            if ( is.null(opt$ident2use ) ){
                print("NO cell identity column name AVAILABLE! The clusters column will be used as default.")
                ident2use = "clusters"
            }else{
                ident2use = opt$ident2use
            }
            cluster_list = unlist(strsplit( opt$which_cells,",",perl = T))
            singlecell_ob = SubsetData(singlecell_ob,cells =
                               OldWhichCells( singlecell_ob, subset.name= ident2use, accept.value = cluster_list))
        }
    }
}

#======Determine statistically significant principal components for clustering=====
if ( reduct1 %in% names(Key(singlecell_ob)) & opt$rerun == F ){#if previous reduction has been calculated ,donot rerun
    if ( reduct1 %in% c("pca","cca","harmony","ica", "lsi") ){ #if the prevous reduction is primary reduction
        #check the components
        # find the optimal components to for secondary reduction
        if ( is.null(Misc(singlecell_ob, "optimal_pc")) ){ #if previous optimal components not available
            print( "NO previous optimal components is AVAILABLE, the optimal components number be detected automatically.")
            # sdev = Stdev(object = singlecell_ob, reduction = reduct1)
            # pct = sdev / sum( sdev ) * 100
            # cum = cumsum(pct)
            # co1 = which(cum > 90 & pct < 5)[1]
            # co2 = sort(which((pct[1:length(pct)-1] - pct[2:length(pct)]) > 0.1),
            #             decreasing = T)[1] + 1 # last point where change of % of variation is more than 0.1%.
            # optimal_pc = ceiling(min(co1, co2)) # change to any other number
            elb = ElbowPlot(singlecell_ob, reduction = reduct1)
            optimal_pc = FindElbow(elb$data)
            suppressWarnings({ Misc(singlecell_ob, "optimal_pc") = optimal_pc})
        }
    }else{ # reduct1 is not primary reduction and previouly run without primary reduction
            # the exception is mnn, it's better to be specified from command line with relative low value
            suppressWarnings({ Misc(singlecell_ob, "components_num") =
                min(opt$components, ncol(Embeddings(singlecell_ob,reduction = reduct1))) })
    }
}else{ # this can be primary reduction or secondary reduction
    print( "NO specified primary reduction found or forced to rerun! \n Reduction begins!")
    message(paste0("Running ", reduct1, " Dimension Reduction"))
    dim_outdir = file.path(output_dir,paste0(reduct1, "_Dimension_Reduction"))
    if ( !dir.exists(dim_outdir) ){
        dir.create(dim_outdir)
    }
    singlecell_ob = RunDimReduc(singlecell_ob, reduct1.use = reduct1 ,
                        reduct2.use = reduct1,
                        feature.use = VariableFeatures(singlecell_ob),
                        perplexity = perplexity,
                        assay.use = DefaultAssay(singlecell_ob),
                        batch.use = batchid, npcs.use = opt$components )
    reduct1_coord = FetchData(singlecell_ob,
                                var = c("orig.ident", paste0( Key(singlecell_ob)[reduct1], 1:2))) %>%
                    dplyr::rename( "Barcode" = "orig.ident")
    #write.table( reduct1_coord, file.path(dim_outdir, paste0(reduct1, "_Dimension_Reduction_coordination.csv")),
                #sep = ",", col.names = T, row.names = F, quote = F)

    if ( reduct1 %in% c("pca","cca","harmony","ica", "lsi") ){ #if the prevous reduction is primary reduction
        if ( !is.null(opt$components) ){
            suppressWarnings({ Misc(singlecell_ob, "components_num") = opt$components })
        }else{
            elb = ElbowPlot(singlecell_ob, reduction = reduct1)
            components_num = FindElbow(elb$data)
            suppressWarnings({ Misc(singlecell_ob, "optimal_pc") = components_num})
        }

    }else if( reduct1 == "mnn"){
        singlecell_ob = RunMnn.Seurat(singlecell_ob, features = NULL,
                                assay = DefaultAssay(singlecell_ob),
                                batch = batchid )
        suppressWarnings({ Misc(singlecell_ob, "components_num") = opt$components })
        suppressWarnings({ Misc(singlecell_ob, "optimal_pc") = opt$components})
    }
}
if ( !is.null(Misc(singlecell_ob, "components_num")) ){ #if specified components available, use it
    components2use = Misc(singlecell_ob, "components_num")
}else{
    components2use =  Misc(singlecell_ob, "optimal_pc")
}

#=================================================================================
#########Clustering with the reduction results using different clustering methods
#=================================================================================
# in case of dimension number out of the actual.
components2use = min( components2use, ncol(Embeddings(singlecell_ob[[reduct1]])))
if ( clusteruse == "snn" ){ #shared nearest neigbours of seurat
    # save.SNN = T saves the SNN so that the clustering algorithm can be rerun
    # using the same graph but with a different resolution value (see docs for full details)
    message(paste0("Beginning to cluster the cell using snn with ",reduct1," reduction results" ) )
    singlecell_ob = FindNeighbors( singlecell_ob, reduction = reduct1, dims = 1:components2use,
                                features = VariableFeatures(singlecell_ob),
                               nn.eps = 0, force.recalc = T, verbose = F)
    singlecell_ob <- FindClusters(object = singlecell_ob, resolution = resolution, algorithm = 1, verbose = F)
    message(paste0("Finished clustering the cell using snn with ",reduct1," reduction results" ) )
}

if ( clusteruse == "leiden" ){ #shared nearest neigbours of seurat
    # using the same graph but with a different resolution value (see docs for full details)
    # To run Leiden algorithm, you must first install the leidenalg python package
    # (e.g. via pip install leidenalg), see Traag et al (2018).
    message(paste0("Beginning to cluster the cell using leidn with",reduct1," reduction results" ) )
    singlecell_ob = FindNeighbors( singlecell_ob, reduction = reduct1, dims = 1:components2use,
                                features = VariableFeatures(singlecell_ob),
                               nn.eps = 0, force.recalc = T, verbose = F)
    singlecell_ob <- FindClusters(object = singlecell_ob, resolution = resolution, algorithm = "leiden", verbose = F)
    message(paste0("Finished clustering the cell using leidn with ",reduct1," reduction results" ) )
}

if ( clusteruse == "infomap" ){
    message(paste0("Beginning to cluster the cell using infomap with ",reduct1," reduction results" ) )
    singlecell_ob = RunInfomap(singlecell_ob, reduction = reduct1,
                                dims = 1:components2use, method= clusteruse)
    message(paste0("Finished clustering the cell using infomap with ",reduct1," reduction results" ) )
}

if ( clusteruse == "hdbscan" ){
    message(paste0("Beginning to cluster the cell using hdbscan with ",reduct1," reduction results" ) )
    singlecell_ob = HDBClust(singlecell_ob, reduction = reduct1, dims = 1:components2use )
    message(paste0("Finished clustering the cell using hdbscan with ",reduct1," reduction results" ) )
}

if ( clusteruse == "louvain" ){
    # save.SNN = T saves the SNN so that the clustering algorithm can be rerun
    # using the same graph but with a different resolution value (see docs for full details)
    message(paste0("Beginning to cluster the cell using lovain with ",reduct1," reduction results" ) )
    singlecell_ob = FindNeighbors( singlecell_ob, reduction = reduct1, dims = 1:components2use,
                                features = VariableFeatures(singlecell_ob),
                               nn.eps = 0, force.recalc = T, verbose = F)
    singlecell_ob <- FindClusters(object = singlecell_ob, resolution = resolution, algorithm = "louvain", verbose = F)
    message(paste0("Finished clustering the cell using lovain with ",reduct1," reduction results" ) )
}

if ( clusteruse == "kmeans" ){
    message(paste0("Beginning to cluster the cell using kmeans with ",reduct1," reduction results" ) )
    # this kmeans will find the optimal cluster numbers internally
    singlecell_ob = DoKMeans( singlecell_ob, dims = 1:components2use,
                    iter.max = 20, use.reduction = reduct1 )
    message(paste0("Finished clustering the cell using kmeans with",reduct1," reduction results" ) )
}

if ( clusteruse == "mbkmeans" ){
    message(paste0("Beginning to cluster the cell using mbkmeans with ",reduct1," reduction results" ) )
    # this mbkmeans will find the optimal cluster numbers internally
    singlecell_ob = DoKMeans( singlecell_ob, dims = 1:components2use, minibatch = T,
                                iter.max = 20, use.reduction = reduct1 )
    message(paste0("Finished clustering the cell using mbkmeans with ",reduct1," reduction results" ) )
}

# one of graph-based methods for cell clustering
# Jacob H. Levine et al, Cell(2015). Data-Driven Phenotypic Dissection of AML Reveals Progenitor-like Cells that Correlate with Prognosis
if ( clusteruse == "phenograph" ){
    suppressPackageStartupMessages( library("ReductionWrappers"))
    message(paste0("Beginning to cluster the cell using phenograph with ",reduct1," reduction results" ) )
    singlecell_ob = DoPhenograph( singlecell_ob, reduction = components2use, k = 30, n_jobs = 10)
    # the -1 in the idents are consider to be outliers
    message(paste0("Finished clustering the cell using phenograph with ",reduct1," reduction results" ) )
}

#=================================================================================
######### secondary reduction
#=================================================================================
# secondary reduction used to detect the community in graph if graph-based clustering
# method used.
if ( !is.null( reduct2) & tolower(reduct2) != "null"){
    message(paste0("Beginning ", reduct2, " Dimension Reduction"))
    output_dir = file.path(output_dir,paste0(reduct2, "_Dimension_Reduction"))
    if ( !dir.exists(output_dir) ){
        dir.create(output_dir)
    }
    singlecell_ob = RunDimReduc(singlecell_ob, reduct1.use = reduct1 , reduct2.use = reduct2,
                            feature.use = VariableFeatures(singlecell_ob),
                            assay.use = DefaultAssay(singlecell_ob),
                            batch.use = batchid, npcs.use = components2use)
    reduct2_coord = FetchData(singlecell_ob,
                        var = c("orig.ident", paste0( Key(singlecell_ob)[reduct2], 1:2))) %>%
                        dplyr::rename( "Barcode" = "orig.ident")
    # write.table( reduct2_coord, file.path(output_dir, paste0(reduct2, "_Dimension_Reduction_coordination.csv")),
    # sep = ",", col.names = T, row.names = F, quote = F)
}else{
    reduct2 = reduct1
}

#=================================================================================
######### validate the clustering results
#=================================================================================
# pdf(file.path(output_dir,"clustering_evaluation_result.pdf") )
# VisClusterCor(singlecell_ob,
#               reduction = "pca",
#               cor_method = "pearson",
#               assay = DefaultAssay(singlecell_ob),
#               prefix = "C",
#               use.average = T)
# dev.off()
# singlecell_ob <- BuildClusterTree(singlecell_ob, reorder.numeric = TRUE, do.reorder = TRUE,
#                             show.progress = F, do.plot = F)
# node.scores <- AssessNodes(singlecell_ob)
# node.scores = node.scores[order(node.scores$oobe, decreasing = TRUE), ]
# max_allowed_oobe = 0.10   # any tree branches exceeding this OOBE are collapsed.
# ## must re-play this section for each merge event, since only one merge per 'play'
# node_to_merge = node.scores$node[node.scores$oobe > max_allowed_oobe]
# if (length(node_to_merge) > 0) {
#     # have at least one node to merge
#     message("merging high OOBE clusters")
#     # do merge for top set:
#     singlecell_ob = Seurat:::MergeNode(object=singlecell_ob, node.use=node_to_merge[1])
#     # now redo the few steps we performed just above - in this new post-merge clustering
#     # examine the new tree post-merge
#     singlecell_ob <- BuildClusterTree(singlecell_ob, do.reorder = TRUE,  show.progress = F,
#     reorder.numeric = TRUE, do.plot =F)
#
#     # reexamine classification error in the context of the updated clusters:
#     node.scores <- AssessNodes(singlecell_ob)
#     node.scores = node.scores[order(node.scores$oobe, decreasing = TRUE), ]
# } else {
#     message("no clusters with high OOBE to merge.")
# }

#=================================================================================
######### save the clustering results
#=================================================================================
cell_id = Idents(singlecell_ob)
new_id = as.numeric(as.vector(cell_id)) + 1 # new cluster id start from 1
names(new_id) = names(cell_id)
new_id = as.factor(new_id)
cell_count_by_cluster = table( new_id )
singlecell_ob = SetIdent(singlecell_ob, value = new_id )
cluster_result_colname = paste(DefaultAssay(singlecell_ob), reduct2,"res",resolution,sep = ".")
#as default the seurat will store the clustering results in the singlecell_object@ident
#to keep the clutering results using the specified resolution to singlecell_object@metadata for reuse
singlecell_ob <- StashIdent(object = singlecell_ob, save.name = cluster_result_colname)
singlecell_ob <- StashIdent(object = singlecell_ob, save.name = "clusters")
reordered_cell_count_by_cluster = table(Idents(singlecell_ob))
cell_count_labels = paste(paste(names(reordered_cell_count_by_cluster),reordered_cell_count_by_cluster,sep="-")," cells")
if(!is.null(opt$species)){
    tsne_clusters_info = singlecell_ob@meta.data %>%
                             dplyr::rename( "Barcode" = "orig.ident") %>%
                             select( Barcode, sampleid, clusters, group )
}

# saveRDSMC(singlecell_ob, file.path(dirname(output_dir), paste0("3d", resolution, ".rds", collapse="")) )
# plot 3D
vis3D = DimPlotly3d(singlecell_ob, reduction= reduct2,
                                        pt_size = as.numeric(opt$pointsize), palette = CustomCol2(1:length(unique(Idents(singlecell_ob)))),
                                        plot_grid = F)
htmlwidgets::saveWidget(vis3D, file = file.path(output_dir,paste0(reduct2,"_resolution",resolution,"_3D_plot.html",collapse="")))

unlink(file.path(output_dir,paste0("tsne_resolution",resolution,"_3D_plot_files")),recursive = T)
